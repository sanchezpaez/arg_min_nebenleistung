{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1353ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_features import get_length, check_opinion_verbs, get_subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eba627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff26323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "\n",
    "SAMPLE_ARTICLES_DIR = 'sample_articles/'\n",
    "\n",
    "# WHOLE DATASET\n",
    "ARTICLES_DIR = 'IBM_Debater_(R)_CE-EMNLP-2015.v3/articles/'\n",
    "\n",
    "\n",
    "def transform_files_to_dataframes(articles_file, claims_file, evidences_file):\n",
    "    # Get text of all articles\n",
    "    articles_dataframe = pd.read_csv(articles_file, sep=\"\t\")\n",
    "    # Get all claims\n",
    "    claims_dataframe = pd.read_csv(claims_file, sep=\"\t\")\n",
    "    # Get all evidences\n",
    "    evidences_dataframe = pd.read_csv(evidences_file, sep=\"\t\")\n",
    "    # Add names of columns to evidence dataframe\n",
    "    evidences_dataframe.columns = ['Topic', 'Claim', 'Evidence', 'Evidence Type']\n",
    "    return articles_dataframe, claims_dataframe, evidences_dataframe\n",
    "\n",
    "# if we do not use evidences_dataframe, maybe delete it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df1ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_labelled_sentences_from_data(articles_file, claims_file, evidences_file):\n",
    "    articles_dataframe, claims_dataframe, evidences_dataframe = transform_files_to_dataframes(\n",
    "        articles_file, claims_file, evidences_file\n",
    "    )\n",
    "    # print(f\"Number of articles {len(articles_dataframe.Title)}\")\n",
    "    # claim_or_text = \"Claim original text\"\n",
    "    # print(f\"Number of claims {len(claims_dataframe.get(claim_or_text))}\")\n",
    "    # ev = \"Evidence\"\n",
    "    # print(f\"Number of evidences {len(evidences_dataframe.get(ev))}\")\n",
    "\n",
    "    directory = os.fsencode('IBM_Debater_(R)_CE-EMNLP-2015.v3/articles/')\n",
    "    article_no_claims = []\n",
    "    number_of_claims = 0\n",
    "    number_of_evidences = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        with open(os.path.join(directory, file), 'r') as txt_file:\n",
    "            txt = txt_file.read().replace('\\n', '')\n",
    "            sentences = nltk.tokenize.sent_tokenize(txt)\n",
    "            # Get topic from article id\n",
    "            art_id = int(filename[6:-4])\n",
    "            topic = articles_dataframe.loc[articles_dataframe['article Id'] == art_id, 'Topic']\n",
    "            if len(topic) > 0:\n",
    "                # Get all claims for this topic\n",
    "                claims_to_topic = claims_dataframe.loc[claims_dataframe['Topic'] == topic.item()]  # select rows from a df based on values in column\n",
    "                list_claims_ori = claims_to_topic['Claim original text'].tolist()\n",
    "                list_claims_cor = claims_to_topic['Claim corrected version'].tolist()\n",
    "                for index, row in claims_to_topic.iterrows():\n",
    "                    claim = row['Claim original text']\n",
    "                    # print(index)\n",
    "                    # print(claim)\n",
    "                    if claim in evidences_dataframe.values:\n",
    "                        evidence = evidences_dataframe.loc[evidences_dataframe['Claim'] == claim, 'Evidence']\n",
    "                        # print(claim, evidence)\n",
    "                        number_of_evidences += 1\n",
    "                for sentence in sentences:\n",
    "                    X.append(sentence)\n",
    "                    if any(s in sentence for s in list_claims_ori) or any(s in sentence for s in list_claims_cor):\n",
    "                        # Label sentence as 'with claim'\n",
    "                        Y.append(1)\n",
    "                        number_of_claims += 1\n",
    "                    else:\n",
    "                        # Label sentence as 'without claim'\n",
    "                        Y.append(0)\n",
    "            else:\n",
    "                article_no_claims.append(art_id)\n",
    "                continue\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e290af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sentences,  labels = get_labelled_sentences_from_data(\n",
    "    'IBM_Debater_(R)_CE-EMNLP-2015.v3/articles.txt', 'IBM_Debater_(R)_CE-EMNLP-2015.v3/claims.txt',\n",
    "    'IBM_Debater_(R)_CE-EMNLP-2015.v3/evidence.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecdee3",
   "metadata": {},
   "source": [
    "# Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d45c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# downsampled_sentences = []\n",
    "# downsampled_labels = []\n",
    "\n",
    "\n",
    "# for i in range(0, len(sentences)):\n",
    "#     if labels[i] == 1:\n",
    "#         downsampled_sentences.append(sentences[i])\n",
    "#         downsampled_labels.append(labels[i])\n",
    "        \n",
    "# not_claims = list(set(sentences) - set(downsampled_sentences))\n",
    "\n",
    "# # we do not need indices of corresponding labels because we know that they are 0\n",
    "# inputNumbers = range(0, len(not_claims))\n",
    "\n",
    "# random_not_cailms = random.sample(inputNumbers , 3000)\n",
    "# for i in random_not_cailms:\n",
    "#     downsampled_sentences.append(not_claims[i])\n",
    "#     downsampled_labels.append(0)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4c6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = downsampled_sentences\n",
    "# labels = downsampled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fcf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from main import stopwords, lemmatize_sentence, lemmatizer, get_wordnet_pos, remove_stopwords\n",
    "from new_version import get_labelled_sentences_from_data\n",
    "\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    X_preprocessed = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        clean_sentence = remove_stopwords(sentence)\n",
    "        # lemmatized_sentence = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in clean_sentence.split()]\n",
    "        lemmatized_sentence = lemmatize_sentence(clean_sentence)  # Doesn't work, returns list of list\n",
    "        X_preprocessed.append(' '.join(lemmatized_sentence))\n",
    "\n",
    "#     save_data(X_preprocessed, 'preprocessed_sentences.txt')\n",
    "    print(f\"The number of preprocessed sentences is {len(X_preprocessed)}.\")\n",
    "#     print(f\"The number of labels is {len(y)}.\")\n",
    "    print(\"The sentences have been saved and will be available as 'preprocessed_sentences.txt'\")\n",
    "\n",
    "    return X_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cba02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 75620/75620 [06:03<00:00, 208.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of preprocessed sentences is 75620.\n",
      "The sentences have been saved and will be available as 'preprocessed_sentences.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sent = preprocess_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc6551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "X = preprocessed_sent\n",
    "y = labels\n",
    "indexes = [i for i in range(len(X))]\n",
    "X, y, indexes = shuffle(X, y, indexes, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8497c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data in train and test\n",
    "X_sent_train, X_rest, y_train, y_rest, orig_train, orig_rest = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        indexes,\n",
    "        test_size=.2,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "# X_rest, y_rest will be used for testing later! \n",
    "# We now need to divide our training set again: into trianing and dev set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5d07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And again, to get dev set\n",
    "X_train_dev, X_test_dev, y_train_dev, y_test_dev, orig_dev, orig_test = train_test_split(\n",
    "        X_sent_train,\n",
    "        y_train,\n",
    "        orig_train,\n",
    "        test_size=.3,\n",
    "        random_state=SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713b2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18149\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27116a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_train = np.array(y_train_dev)\n",
    "y_dev = np.array(y_test_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5c770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bd44446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_length(sentence):\n",
    "#     # or true if longer than 4 words?\n",
    "#     if len(sentence) > 4:\n",
    "#         return 1\n",
    "#     return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493984f",
   "metadata": {},
   "source": [
    "# Preparing feture matrix for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3d29b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The firts column of the feature matrix is 0 if sent lenth is <= 4, otherwise 1\n",
    "\n",
    "# for i in range(len(X_sent_train)):               \n",
    "#     X_train[i, 0] = get_length(word_tokenize(X_sent_train[i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8fa8df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second column of the feature matrix is 0 if there are no opinion verbs in the sentence \n",
    "# and is 1 if there is at least 1 opinion verbs in the sentence.\n",
    "\n",
    "# for i in range(len(X_sent_train)):               \n",
    "#     X_train[i, 1] = check_opinion_verbs(word_tokenize(X_sent_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d6ac508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third column of the feature matrix contains subjectivity scores.\n",
    "\n",
    "# for i in range(len(X_sent_train)):               \n",
    "#     X_train[i, 2] = get_subjectivity_score(word_tokenize(X_sent_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2ca53",
   "metadata": {},
   "source": [
    "# Preparing feture matrix for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3abb7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_rest = np.zeros((len(X_sent_rest), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d52a81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The firts column of the feature matrix is 0 if sent lenth is <= 4, otherwise 1\n",
    "\n",
    "# for i in range(len(X_sent_rest)):               \n",
    "#     x_rest[i, 0] = get_length(word_tokenize(X_sent_rest[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8e213e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(X_sent_rest)):               \n",
    "#     x_rest[i, 1] = check_opinion_verbs(word_tokenize(X_sent_rest[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a70efca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(X_sent_rest)):               \n",
    "#     x_rest[i, 2] = get_subjectivity_score(word_tokenize(X_sent_rest[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422599e",
   "metadata": {},
   "source": [
    "# # Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "501238bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7cfd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42347, 34770)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3a1f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2dca0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = vectorizer.transform(X_test_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e26a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from sklearn.svm import SVC\n",
    "\n",
    "# svm = SVC(kernel='linear', random_state=SEED, max_iter=25)\n",
    "# svm.fit(X_train, y_train)\n",
    "# binary_balanc_predictions = svm.predict(x_dev)\n",
    "# # binary_balanc_probs = svm.predict_proba(x_dev)\n",
    "# print(f'Accuracy on the training set: {svm.score(X_train, y_train)}')\n",
    "# print(f'Accuracy on the test set: {svm.score(x_dev, y_dev)}')\n",
    "\n",
    "# print(\"SVM Accuracy Score ->\", accuracy_score(binary_balanc_predictions,y_dev) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "774cbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.9626556016597511\n",
      "Accuracy on the test set: 0.7596774193548387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(C =1.0, kernel='linear', degree = 3, gamma = 'auto', probability =True, random_state=SEED)\n",
    "svm.fit(X_train, y_train)\n",
    "binary_balanc_predictions = svm.predict(x_dev)\n",
    "# binary_balanc_probs = svm.predict_proba(x_dev)\n",
    "print(f'Accuracy on the training set: {svm.score(X_train, y_train)}')\n",
    "print(f'Accuracy on the test set: {svm.score(x_dev, y_dev)}')\n",
    "\n",
    "# print(\"SVM Accuracy Score ->\", accuracy_score(binary_balanc_predictions,y_dev) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4449ec72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'auto',\n",
       " 'kernel': 'linear',\n",
       " 'max_iter': -1,\n",
       " 'probability': True,\n",
       " 'random_state': 10,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79aa9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this line (of course, modifying the path)\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/Margot/Desktop/DataScience/_functions_/_functions_/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5fc374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import plotly.express as px\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from string import ascii_uppercase\n",
    "from pandas import DataFrame\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a503ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/olhasvezhentseva/prs-env/lib/python3.7/site-packages (from sklearn_crfsuite) (4.64.0)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.8-cp37-cp37m-macosx_10_9_x86_64.whl (180 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six in /Users/olhasvezhentseva/prs-env/lib/python3.7/site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Installing collected packages: python-crfsuite, tabulate, sklearn_crfsuite\n",
      "Successfully installed python-crfsuite-0.9.8 sklearn_crfsuite-0.3.6 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn_crfsuite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93d608a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Define the param_grid \n",
    "param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n",
    "              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n",
    "              'kernel': ['rbf', 'poly', 'linear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0eada01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={'C': [0.01, 0.1, 0.5, 1, 10, 100],\n",
       "                         'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf', 'poly', 'linear']})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. GridSearch and fit the model\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ceade8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.75, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params= grid.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f3529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# param_grid={'C': [0.01, 0.1, 0.5, 1, 10, 100],\n",
    "#                          'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001],\n",
    "#                          'kernel': ['rbf', 'poly', 'linear']})\n",
    "\n",
    "\n",
    "\n",
    "#1) BEST PARAM = {'C': 10, 'gamma': 0.75, 'kernel': 'rbf'}\n",
    "# if cv = 3\n",
    "\n",
    "# Accuracy on the training set: 1.0\n",
    "# Accuracy on the test set: 0.7758064516129032  but C = 3, 6 the same\n",
    "\n",
    "\n",
    "#2) BEST PARAM = {'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\n",
    "# if cv = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e6e6752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params1 = grid.best_params_\n",
    "best_params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "124a9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 1.03, 1: 34.4}\n",
    "# I got the weight by getting the prob, for example prob of class 0 is  41117 /(1230+41117), \n",
    "# the inverse probability is 1/ 41117 /(1230+41117), which is 1.03.\n",
    "\n",
    "#  The alternative is to give \"balanced\" to the parameter \"class_weight\", the re.lation between the weights will\n",
    "#  the same, for example 0.5 for class 0 and 17 for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "76ffaceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.9988665076628805\n",
      "Accuracy on the test set: 0.9725053721968152\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 'rbf' to linear\n",
    "\n",
    "# # class_weight = 'balanced'\n",
    "\n",
    "svm2 = SVC(C = 10, gamma= 0.75, kernel = 'rbf', class_weight = weights, random_state=SEED)\n",
    "svm2.fit(X_train, y_train)\n",
    "binary_balanc_predictions = svm2.predict(x_dev)\n",
    "# binary_balanc_probs = svm.predict_proba(x_dev)\n",
    "print(f'Accuracy on the training set: {svm2.score(X_train, y_train)}')\n",
    "print(f'Accuracy on the test set: {svm2.score(x_dev, y_dev)}')\n",
    "\n",
    "# print(\"SVM Accuracy Score ->\", accuracy_score(binary_balanc_predictions,y_dev) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c68e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.03 34.4 ]\n"
     ]
    }
   ],
   "source": [
    "print(svm2.class_weight_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e3deee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.9754173849387205\n",
      "Accuracy on the test set: 0.9715686814700535\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # class_weight = 'balanced'\n",
    "\n",
    "# svm2 = SVC(random_state=SEED)\n",
    "# svm2.fit(X_train, y_train)\n",
    "# binary_balanc_predictions = svm2.predict(x_dev)\n",
    "# # binary_balanc_probs = svm.predict_proba(x_dev)\n",
    "# print(f'Accuracy on the training set: {svm2.score(X_train, y_train)}')\n",
    "# print(f'Accuracy on the test set: {svm2.score(x_dev, y_dev)}')\n",
    "\n",
    "# # print(\"SVM Accuracy Score ->\", accuracy_score(binary_balanc_predictions,y_dev) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3bf86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# mlp = MLPClassifier(verbose=True, random_state=SEED, max_iter=15)\n",
    "# mlp.fit(X_train, y_train)\n",
    "# binary_balanc_predictions = mlp.predict(x_dev)\n",
    "# binary_balanc_probs = mlp.predict_proba(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6097a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy on the training set: {mlp.score(X_train, y_train)}')\n",
    "print(f'Accuracy on the test set: {mlp.score(x_dev, y_dev)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70222404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_balanc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9339acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(binary_balanc_predictions)):\n",
    "    pred_label = binary_balanc_predictions[i]\n",
    "    real_label = y_rest[i]\n",
    "    if pred_label != real_label:\n",
    "        # get the original raw text with the corresponding index\n",
    "        orig_index = orig_rest[i]\n",
    "        print(f'Inorrect {pred_label} at sentence: \\n    {sentences[orig_index]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9fd70793",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "classification_report() missing 1 required positional argument: 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d4/lqp28dq549g71qydx4ln89y00000gn/T/ipykernel_61138/1766364286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(classification_report(y_dev, binary_balanc_predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: classification_report() missing 1 required positional argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             precision_recall_fscore_support, ConfusionMatrixDisplay,\n",
    "                             classification_report, confusion_matrix, RocCurveDisplay)\n",
    "\n",
    "# print(classification_report(y_dev, binary_balanc_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b14acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaadaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
